{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "03cd2618a86d57bdf1e13ac9fea40245a5e8eaf90cdbb6415b3fe95ffa9984c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Science Project - Detecting Fraudulent Credit Card Transactions\n",
    "\n",
    "The research question here is to investigate whether we can determine a credit card transation to be fraudulent, using the Credit Card Fraud Detection dataset from Kaggle.\n",
    "\n",
    "First we need to import necessary libraries and load in the data. Then do some early exploratory data analysis to better understand the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Identify whether a credit card transaction is fraudulent or not. Using credit card transaction data from Kaggle \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import imblearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics\n",
    "\n",
    "# Load the test and training data\n",
    "train_raw_df = pd.read_csv(\".\\Data\\creditcard_train.csv\")\n",
    "test_raw_df = pd.read_csv(\".\\Data\\creditcard_test.csv\")\n",
    "\n",
    "# See how many rows and columns there are\n",
    "train_raw_df.shape\n",
    "test_raw_df.shape\n",
    "\n",
    "# Look for null values and make sure data types are matching\n",
    "print(train_raw_df.info())\n",
    "print(test_raw_df.info())\n",
    "\n",
    "# Get a brief visual look at the actual values in the data and make some initial deductions\n",
    "train_raw_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "First inspection seems to show that we are dealing with numerical data in our 30 features, and a categorical label in our 'Class' column with just two classes \"1\" and \"0\". \n",
    "\n",
    "There are fortunately no null or missing values in either training or test set.\n",
    "\n",
    "We can also see that features 'V1 - V28' might already been feature scaled in some way, where as 'Time' and 'Amount' have not. Let's use .describe() on every column to check this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_raw_df:\n",
    "    print(train_raw_df[column].describe(), \"\\n\")"
   ]
  },
  {
   "source": [
    "The prints from above confirm these initial thoughts, because the mean for all of the columns from 'V1' to 'V28' are extremely close to zero, suggesting that the data has been standardized (z-score normalised). \n",
    "\n",
    "It therefore makes sense to use this same type of normalisation on the non-feature scaled features, 'Time' and 'Amount' but only when we are using ML models to classify the data. \n",
    "\n",
    "In the meantime, let us continue with further exploratory data analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicate values\n",
    "print(\"Train duplicates:\", train_raw_df.duplicated().sum())\n",
    "print(\"Test duplicates:\", test_raw_df.duplicated().sum())\n",
    "\n",
    "train_duplicates = train_raw_df[train_raw_df.duplicated()]\n",
    "train_duplicates.sort_values(\"Time\")"
   ]
  },
  {
   "source": [
    "It appears we have a number of duplicates in our datasets. Unfortunately, our data does not contain a clearly identifiable primary key such as 'Transaction ID'. If that was the case then we could simply remove duplicates which shared the same transaction ID.\n",
    "\n",
    "Looking at the documentation for the dataset (Source: https://www.kaggle.com/mlg-ulb/creditcardfraud) we can see that V1-V28 were likely to have been anonymised for the sake of protecting user's identity. This means that it is likely that the values in these fields combined could be enough to uniquely identity a person. Therefore, it makes it highly improbable for all the values in V1-28 AS WELL AS the values in time and amount to all be exactly the same in more than one entry. On this basis, it seems sensible to remove the duplicate values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate data\n",
    "train_df = train_raw_df.drop_duplicates()\n",
    "test_df = test_raw_df.drop_duplicates()\n",
    "print(train_raw_df.shape)\n",
    "print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Class' colum from int64 to category as we know it is a categorical variable\n",
    "test_df[\"Class\"] = test_df['Class'].astype('category')\n",
    "train_df[\"Class\"] = train_df['Class'].astype('category')\n",
    "train_df[\"Class\"]\n"
   ]
  },
  {
   "source": [
    "\n",
    "Now we should try to identify influencial variables by performing futher exploratory analysis whilst also cleaning up the data.\n",
    "\n",
    "Earlier we saw a very high value for the 'max' in the amount column so let's take a lot at the distribution of data in this column.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Amount column\n",
    "fig = px.box(train_df, y=\"Amount\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the two largest values for Amount\n",
    "outliers = train_df.nlargest(2, \"Amount\")\n",
    "# Since these outliers are part of the majority class, getting rid of them should not have a negative effect\n",
    "train_df = train_df[~train_df.isin(outliers)].dropna(how=\"all\")\n",
    "fig = px.box(train_df, y=\"Amount\", title=\"Boxplot for Amount\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distribution of the other features\n",
    "fig = px.box(train_df, x=[\"V\" + str(i) for i in range(1,29)], title=\"Boxplot of all the anonymous features\")\n",
    "#fig.show() don't show due to computer lag\n"
   ]
  },
  {
   "source": [
    "## Removal of outliers\n",
    "\n",
    "Since a few of the anonymous features appear to have a couple outliers, we can go through all of them and drop the two largest and two smallest, but we should only do this if they are memebr of the majority class (Class=0) since this is something we will be doing anyway when we undersample."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the majority class from the train_df\n",
    "class_0_train_df = train_df[train_df[\"Class\"] == 0]\n",
    "\n",
    "# Go through the columns of the df\n",
    "for series in class_0_train_df:\n",
    "    if series != \"Time\" and series != \"Amount\" and series != \"Class\":\n",
    "        # Find the upper and lower 0.1%\n",
    "        q_low = class_0_train_df[series].quantile(0.001)\n",
    "        q_hi  = class_0_train_df[series].quantile(0.999)\n",
    "        # Remove these extreme values\n",
    "        no_outlier_train_df = class_0_train_df[(class_0_train_df[series] < q_hi) & (class_0_train_df[series] > q_low)]\n",
    "\n",
    "# Add all the minority class examples back in\n",
    "no_outlier_train_df = no_outlier_train_df.append(train_df[train_df[\"Class\"] == 1])\n",
    "\n",
    "print(no_outlier_train_df.shape)\n",
    "print(train_df.shape)\n",
    "   "
   ]
  },
  {
   "source": [
    "Now we want perform dimensionality reduction since we have a large number of dimensions. PCA will help to reduce the dimensions and enable us to visualise the data better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardise \"Time\" and \"Amount\" since the other features are already standardised\n",
    "norm_train_df = no_outlier_train_df\n",
    "def standardise(X):\n",
    "    X_std = X - np.mean(X, axis=0)      # Subtract the mean of the feature from each example\n",
    "    X_std = np.divide(X_std, np.std(X_std, axis=0))     # Divide each example by the range of the feature\n",
    "    return X_std\n",
    "\n",
    "norm_train_df[\"Time\"] = standardise(no_outlier_train_df[\"Time\"])\n",
    "norm_train_df[\"Amount\"] = standardise(no_outlier_train_df[\"Amount\"])\n",
    "# Separate X and Y from dataset\n",
    "train_X_df = norm_train_df.iloc[:, :-1]\n",
    "train_y_df = norm_train_df[\"Class\"]\n",
    "train_X, train_y = np.array(train_X_df), np.array(train_y_df)\n",
    "# Compute covariance matrix\n",
    "cov_mat = np.matmul(train_X.T, train_X)\n",
    "# Compute eigenvectors and eigenvalues\n",
    "eig_val, eig_vec = np.linalg.eig(cov_mat)\n",
    "print(\"Eigenvectors:\", eig_vec, \"\\nEigenvalues:\", eig_val)"
   ]
  },
  {
   "source": [
    "Now we can find out how much each eigenvector contributes to the overall variance by comparing the size of their eigenvalues. This can also be shown in a plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of code taken from https://github.com/AI-Core/Practical-ML-DS/blob/master/Chapter%202.%20Python%20for%20Data%20Scientists/Module%205.%20Unsupervised%20Feature%20Representations/0.%20PCA%20and%20t-SNE.ipynb\n",
    "\n",
    "var_percent = [(i/sum(eig_val)) * 100 for i in eig_val]   # calculate the percentage variance of the data which this eigenvalue accounts for\n",
    "cum_var_percent = np.cumsum(var_percent)    # make a vector of the cumulative sum of the variance percentages\n",
    "\n",
    "fig = plt.figure(figsize=(22,10))      \n",
    "ax =  fig.add_subplot(111)      # add an axis\n",
    "plt.title('Variance along principal components')\n",
    "ax.grid()\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Percentage total variance accounted for')\n",
    "\n",
    "ax.plot(cum_var_percent, '-ro')     # plot the cumulative sum of the variances accounted for by each eigenvector\n",
    "ax.bar(range(len(eig_val)), var_percent) # position, height # show how much variance individual eig accounts for\n",
    "plt.xticks(np.arange(len(eig_val)), ('PC{}'.format(i) for i in range(len(eig_val))))  # set the xticks to 'PC1' etcx\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data to 3 dimensions\n",
    "W = eig_vec[:, :3]\n",
    "reduced_train_X = np.matmul(train_X, W)\n",
    "reduced_train_X\n"
   ]
  },
  {
   "source": [
    "Now we can plot the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign different colours to each class\n",
    "colour_dict = {0:'r', 1:'b'}     \n",
    "colour_list = [colour_dict[i] for i in list(train_y)] \n",
    "\n",
    "fig = plt.figure(figsize=(25,15))\n",
    "plt.grid()\n",
    "ax = fig.add_subplot(111, projection='3d')      # add a 3d set of axes\n",
    "ax.scatter(reduced_train_X[:, 0], reduced_train_X[:, 1], reduced_train_X[:, 2], c=colour_list)    # scatter plot our 3d data\n",
    "plt.xlabel('PC1 value')\n",
    "plt.ylabel('PC2 value')\n",
    "ax.set_zlabel('PC3 value')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Imbalanced Data - Undersampling vs Oversampling\n",
    "\n",
    "Imbalanced data is when the distribution of classes is uneven, and there is a clear majority and minority class. Undersampling involves removing examples in the majority class to help balance the data, whereas oversampling involves duplicating example from the minority class. The examples to be removed or duplicated are selected at random, which is why these methods are types of random resampling. These naive methods also assume nothing about the data, which makes them fast and easy to use as no heuristics are used.  (Source: https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/)\n",
    "\n",
    "Random oversampling increases the chance of overfitting to the minority class, as the number of their cases have been artificially inflated. In contrast, random undersampling increases the chance to reduce the performance of your model as you might lose key information-rich examples from the dataset.\n",
    "\n",
    "## Identify and discuss at least 2 suitable evaluation metrics for this task. Then classify the data.\n",
    "\n",
    "Since we are working with imbalanced data, it does not make sense to use accuracy as an evaluation metric. This is because if the system just predicted everything to be negative (i.e. class = 0) it would still get a high accuracy score. Instead we should look at precision, recall, and the F1 score which is a combination of the previous two. \n",
    "\n",
    "Since we have a low number of overall positive cases (i.e. where the class = 1), recall and F1 score will be the two most important metrics here and therefore what will be taken into account. This is because recall is the number of true positives divided by the total number of actual positives in the dataset. This means it will give a low score if the model gives a lot of false negatives. F1 score is good because it combines both precision and recall to give a good overall score. (Source: https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the test dataset too\n",
    "norm_test_df = test_df\n",
    "norm_test_df[\"Time\"] = standardise(test_df[\"Time\"])\n",
    "norm_test_df[\"Amount\"] = standardise(test_df[\"Amount\"])\n",
    "\n",
    "# separate into X and labels\n",
    "test_X_df = norm_test_df.iloc[:,:-1]\n",
    "test_y_df = norm_test_df[\"Class\"]\n",
    "\n",
    "# Find out the current ratio of minority to majority class examples\n",
    "print(np.sum(train_y == 0)/np.sum(train_y == 1)) \n",
    "# Use the Imblearn library to resample the dataset\n",
    "over = imblearn.over_sampling.RandomOverSampler(sampling_strategy=0.02) # Oversample until the ratio is 1:50 or 0.02\n",
    "resampled_X, resampled_y = over.fit_resample(train_X, train_y)\n",
    "\n",
    "under = imblearn.under_sampling.RandomUnderSampler(sampling_strategy=0.5) # Undersample until the ratio is 1:2 or 0.5\n",
    "resampled_X, resampled_y = under.fit_resample(resampled_X, resampled_y)\n",
    "\n",
    "# print new ratio of minority to majority class (should be 2 majority class cases for every 1 minority class case)\n",
    "print(np.sum(resampled_y == 0)/np.sum(resampled_y == 1)) \n",
    "\n"
   ]
  },
  {
   "source": [
    "Now that we have resampled our dataset to make it more balanced, and have also normalised the data earlier, we are ready to run some models to classify the data. The model's perfomance will be based on recall and F1 score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation dataset from the version of the training data BEFORE resampling\n",
    "dont_use_X, valid_X, dont_use_y, valid_y = train_test_split(train_X, train_y)\n",
    "\n",
    "# Logistic regression\n",
    "model1, model2, model3 = LogisticRegression(solver=\"sag\"), LogisticRegression(solver=\"saga\"), LogisticRegression(solver=\"liblinear\")\n",
    "for model in [model1, model2, model3]:\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(valid_X)\n",
    "    print(\"Recall score:\", sklearn.metrics.recall_score(valid_y, pred_y))\n",
    "    print(\"F1 score:\", sklearn.metrics.f1_score(valid_y, pred_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests\n",
    "for max_depth in range(1,10):\n",
    "    model = RandomForestClassifier(max_depth=max_depth)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(valid_X)\n",
    "    print(\"With trees of max depth =\", max_depth, \"\\nRecall score:\", sklearn.metrics.recall_score(valid_y, pred_y))\n",
    "    print(\"F1 score:\", sklearn.metrics.f1_score(valid_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K nearest neighbours\n",
    "for n_neighbours in range(1,20):\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbours)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(valid_X)\n",
    "    print(\"With number of nearest neighbours =\", n_neighbours, \"\\nRecall score:\", sklearn.metrics.recall_score(valid_y, pred_y))\n",
    "    print(\"F1 score:\", sklearn.metrics.f1_score(valid_y, pred_y))\n"
   ]
  },
  {
   "source": [
    "Of the models tried, the best performing model was random forests with a max depth of 9. Let's now try this on the test data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=9)\n",
    "model.fit(train_X, train_y)\n",
    "pred_y = model.predict(test_X)\n",
    "print(\"With trees of max depth =\", max_depth, \"\\nRecall score:\", sklearn.metrics.recall_score(test_y, pred_y))\n",
    "print(\"F1 score:\", sklearn.metrics.f1_score(test_y, pred_y))\n",
    "\n",
    "# Show these results in a confusion matrix\n",
    "sklearn.metrics.plot_confusion_matrix(model, test_X, test_y)\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "## Identify the top 8 most influential variables in the dataset.\n",
    "\n",
    "This can be done by looking at the PCA graph already shown earlier and looking at the 8 largest bars representing how much variance each variable accounts for\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the plot of the PCAs from earlier\n",
    "\n",
    "var_percent = [(i/sum(eig_val)) * 100 for i in eig_val]   # calculate the percentage variance of the data which this eigenvalue accounts for\n",
    "cum_var_percent = np.cumsum(var_percent)    # make a vector of the cumulative sum of the variance percentages\n",
    "\n",
    "fig = plt.figure(figsize=(22,10))      \n",
    "ax =  fig.add_subplot(111)      # add an axis\n",
    "plt.title('Variance along principal components')\n",
    "ax.grid()\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Percentage total variance accounted for')\n",
    "\n",
    "ax.plot(cum_var_percent, '-ro')     # plot the cumulative sum of the variances accounted for by each eigenvector\n",
    "ax.bar(range(len(eig_val)), var_percent) # position, height # show how much variance individual eig accounts for\n",
    "plt.xticks(np.arange(len(eig_val)), ('PC{}'.format(i) for i in range(len(eig_val))))  # set the xticks to 'PC1' etcx\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Here we can see that the first 8 principal components account for most of the variance. Therefore this means that the 8 most influencial variables are the first 8: Time, V1, V2, V3, V4, V5, V6, V7 (in descending order of importance)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}